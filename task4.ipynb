{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28091883-e3ec-45d3-ae7c-d068e2691839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\kiran\\anaconda3\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\kiran\\anaconda3\\lib\\site-packages (4.55.3)\n",
      "Requirement already satisfied: accelerate in c:\\users\\kiran\\anaconda3\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\kiran\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.34.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.7.14)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kiran\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~sspec (C:\\Users\\kiran\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~sspec (C:\\Users\\kiran\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~sspec (C:\\Users\\kiran\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers transformers accelerate PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e30fc74-a463-453a-a6a8-8e844eef2df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from PyPDF2 import PdfReader\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caa60bd1-1584-48fe-9d88-ba4d758e4dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(path):\n",
    "    reader = PdfReader(path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "docs = load_pdf(\"AI_ML Engineering â€“ Advanced Internship Tasks.pdf\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9d2f28b-f11f-4031-9f8d-8f9be5b98925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 2\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size=500, overlap=100):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i+chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(docs)\n",
    "print(f\"Total chunks: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46c2d461-7a47-4ca6-8b23-a2263eee77bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a641020bce24b6b8fb9fc033cce1164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedder.encode(chunks, convert_to_numpy=True, show_progress_bar=True)\n",
    "embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d94787b0-073a-42b4-a65c-6f350ed00e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, k=3):\n",
    "    q_emb = embedder.encode([query], convert_to_numpy=True)[0]\n",
    "    q_emb = q_emb / np.linalg.norm(q_emb)\n",
    "    scores = np.dot(embeddings, q_emb)\n",
    "    top_k_idx = np.argsort(scores)[-k:][::-1]\n",
    "    return [chunks[i] for i in top_k_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d5fe7ef-ee3d-4c54-bbde-90cf8b434594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326d574562c54c58a060396b0328d8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kiran\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kiran\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31f5ae7c5ea44a7afc70c2ddf09ec51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f288d659a7224d2b95157a0364711c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3183263ef97e46b4888414bd6d940ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1409d1a3f6434b7f8cc4e393234e5614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af76b5735d2847009d2c861bd0180333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b105deafb50402aaa96bef665815dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Load a small model (distilgpt2 is light and fast)\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\", device=0 if torch.cuda.is_available() else -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "717ff4f5-99cb-49b3-9341-5f657e24090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "def chatbot(query, history=chat_history, k=3):\n",
    "    retrieved = retrieve(query, k)\n",
    "    context = \"\\n\\n\".join(retrieved)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Use the CONTEXT to answer the QUESTION.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "HISTORY:\n",
    "{history}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "Answer in a clear and simple way:\n",
    "\"\"\"\n",
    "\n",
    "    response = generator(prompt, max_length=250, num_return_sequences=1, do_sample=True)\n",
    "    answer = response[0]['generated_text'].split(\"Answer in a clear and simple way:\")[-1].strip()\n",
    "\n",
    "    history.append((query, answer))\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "615b5b69-7e68-4458-8b80-b52b233e1686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=250) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=250) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": The above information is strictly for general purposes only.\n",
      "\n",
      "What are the steps for the project?\n",
      "This project has been running for six months, and the project is designed to give you more efficient, fast, and agile development. The goal for the project is to give you more efficient, fast, and agile development. The goal for the project is to give you more efficient, fast, and agile development. The goal for the project is to give you more efficient, fast, and agile development. The goal for the project is to give you more efficient, fast, and agile development. The goal for the project is to give you more efficient, fast, and agile development. The goal for the project is to give you more efficient, fast, and agile development. The goal for the project is to give you more efficient, faster, and agile development. The goal for the project is to give you more efficient, fast, and agile development. The goal for the project is to give you more efficient, fast, and agile development. The goal for the project is to give you more efficient, fast, and agile development. The goal for the project is to give you more efficient, faster, and agile development. The goal for the project is to give you more\n",
      "dataset (e.g., BERT) to generate an object-level model for the input dataset (GMD).\n",
      "These tasks are designed to give you hands-on experience with cutting-edge machine learning and artificial intelligence techniques, including transformer models, ML pipelines, multimodal learning, conversational AI, and LLM applications. You are encouraged to complete all tasks to strengthen your technical portfolio. Technologies and tools you'll use include: Hugging Face Transformers, scikit-learn, LangChain, Streamlit, Gradio, CNNs, joblib, and LLM applications.\n",
      "This task is designed to give you hands-on experience with cutting-edge machine learning and artificial intelligence techniques, including transformer models, ML pipelines, multimodal learning, conversational AI, and LLM applications. You are encouraged to complete all tasks to strengthen your technical portfolio. Technologies and tools you'll use include: Hugging Face Transformers, scikit-learn, LangChain, Streamlit, Gradio, CNNs, joblib, and LLM applications. You are encouraged to complete all tasks to strengthen your technical portfolio. Technologies and tools you'll use include: Hugging Face Transformers, scikit-learn, LangChain, Streamlit, Gradio\n"
     ]
    }
   ],
   "source": [
    "print(chatbot(\"Summarize the document in 3 lines.\"))\n",
    "print(chatbot(\"What is the main idea?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4f31b7-344b-4544-9fd3-e74ad1fa1e51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
